# RobotsTxt Feature - Implementation Summary

## Overview

This document summarizes the implementation of the robots.txt creator feature for StaticForge, which automatically generates a robots.txt file based on content metadata.

## What Was Delivered

### Core Feature Implementation
- **File**: `src/Features/RobotsTxt/Feature.php`
- **Purpose**: Automatically generates robots.txt during site build
- **Events**: 
  - POST_GLOB (priority 150): Scans files for robots metadata
  - POST_LOOP (priority 100): Generates robots.txt file

### Features
1. **Individual Page Control**: Honor `robots=yes/no` in content frontmatter
2. **Category Control**: Honor `robots=no` in category definition files to disallow entire categories
3. **Smart Defaults**: Pages without `robots` field default to "yes" (allowed)
4. **Case Insensitive**: Handles `NO`, `No`, `no` consistently
5. **Both MD and HTML**: Works with both Markdown and HTML files
6. **Sorted Output**: Paths are alphabetically sorted for consistency
7. **Sitemap Reference**: Automatically includes sitemap.xml URL from SITE_BASE_URL

### Testing
- **Unit Tests**: 17 test cases covering all functionality
- **Integration Tests**: 6 test cases for end-to-end workflows
- **Test Coverage**: All edge cases including case sensitivity, missing fields, nested paths, category disallow

### Documentation
- **Updated**: `docs/FEATURES.md`
- **Includes**: Complete usage examples, configuration details, generated output examples
- **Added**: Feature comparison table entry

## Usage Examples

### Disallow Individual Page

```markdown
---
title = "Private Page"
robots = "no"
---

# Private Content
This page won't be crawled by search engines.
```

### Allow Individual Page (Explicit)

```markdown
---
title = "Public Page"
robots = "yes"
---

# Public Content
This page can be crawled.
```

### Allow Individual Page (Default)

```markdown
---
title = "Public Page"
---

# Public Content
Default behavior - this page can be crawled.
```

### Disallow Entire Category

```markdown
---
type = "category"
category = "private-stuff"
title = "Private Category"
robots = "no"
---

# Private Category
All files in this category will be disallowed.
```

## Generated Output

### When Pages Are Disallowed

```
# robots.txt generated by StaticForge
# 2025-01-15 10:30:00

User-agent: *
Disallow: /private.html
Disallow: /secret.html
Disallow: /private-stuff/

# Sitemap location
Sitemap: https://example.com/sitemap.xml
```

### When All Pages Are Allowed

```
# robots.txt generated by StaticForge
# 2025-01-15 10:30:00

User-agent: *
# No disallowed paths
Disallow:

# Sitemap location
Sitemap: https://example.com/sitemap.xml
```

## How It Works

1. **Scan Phase**: During POST_GLOB event, the feature scans all discovered content files
2. **Parse Metadata**: Extracts `robots` field from INI-style frontmatter in both MD and HTML
3. **Collect Paths**: Builds a list of paths to disallow (robots=no)
4. **Category Support**: Also scans for category definition files with `type=category` and `robots=no`
5. **Generate File**: During POST_LOOP event, generates robots.txt in output directory
6. **Sort & Dedupe**: Paths are sorted alphabetically and deduplicated
7. **Add Sitemap**: Includes sitemap reference if SITE_BASE_URL is configured

## Configuration

No configuration needed! The feature uses:
- `SOURCE_DIR` - to find content files (default: "content")
- `OUTPUT_DIR` - where to write robots.txt (default: "output")
- `SITE_BASE_URL` - for sitemap reference (optional)

All from your `.env` file.

## Implementation Details

### Frontmatter Parsing

**Markdown Files** (--- ... ---):
```markdown
---
title = "My Page"
robots = "no"
---
```

**HTML Files** (<!-- INI ... -->):
```html
<!-- INI
title = "My Page"
robots = "no"
-->
```

### Category Detection

Files with `type=category` in frontmatter are treated as category definitions:
```markdown
---
type = "category"
category = "blog"
robots = "no"
---
```

This will add `Disallow: /blog/` to robots.txt.

### Path Calculation

- Paths are relative to SOURCE_DIR
- File extensions are converted to .html
- Directory separators are normalized to /
- Nested directories are preserved

Example:
- Input: `content/blog/private-post.md`
- Output: `Disallow: /blog/private-post.html`

## Code Quality

The implementation follows StaticForge's coding standards:
- PSR-12 code style
- PHP 8+ modern syntax (str_contains, str_starts_with, array destructuring)
- Null safety with coalescing operators
- Comprehensive error handling
- Meaningful logging at appropriate levels

## Testing Files Included

Test content files for manual verification:
- `content/public.md` - Example allowed page
- `content/private.md` - Example disallowed page
- `content/secret-category.md` - Example category disallow

## Next Steps

1. Run site generation: `php bin/console.php render:site`
2. Check `output/robots.txt` for generated file
3. Test with your own content files
4. Deploy robots.txt with your site

## Support

See `docs/FEATURES.md` for complete documentation of all features including robots.txt.
